{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LB 0.633349\n",
    "v4版疑惑\n",
    "1. v4: df_full->FE try df_train -> FE ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,cross_validate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "#                               GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "# import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "path_raw_data = '../raw_data'\n",
    "# path_processed_data = '../Data/processed'\n",
    "# path_prediction_data = '../Data/predictions'\n",
    "col_target = 'fraud_ind'\n",
    "col_id = 'txkey'\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "df_train = pd.read_csv(os.path.join(path_raw_data, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(path_raw_data, 'test.csv'))\n",
    "time_end = time.time()\n",
    "print('spent: {}'.format(time_end-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_id = df_train[col_id]\n",
    "df_test_id = df_test[col_id]\n",
    "target = df_train[col_target]\n",
    "\n",
    "df_train.drop([col_id], axis = 1, inplace = True)\n",
    "df_test.drop([col_id], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates(keep = 'first')\n",
    "ntrain = df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat((df_train,df_test),sort = False).reset_index(drop = True)\n",
    "print(df_train.shape, df_test.shape, df_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate time feature ( format hhmmss)\n",
    "df_full['loctm_time'] = df_full['loctm'].apply(lambda x: '0'*(6 - len(str(int(x)))) + str(int(x)))\n",
    "df_full['loctm_time'] = df_full['loctm_time'].apply(lambda x: x[:2] + ':' + x[2:4] + ':' + x[4:])\n",
    "df_full['loctm_time'] = pd.to_datetime(df_full['loctm_time'], format='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.insert(3,'time_hour', df_full['loctm_time'].dt.hour)\n",
    "df_full.insert(3,'time_min', df_full['loctm_time'].dt.minute)\n",
    "df_full.insert(3,'time_sec', df_full['loctm_time'].dt.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 generate columns using conam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Groupby bacno, locdt, loctm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這版先把4.1.1拿掉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Groupby bacno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bycol_num = 1\n",
    "df_full_groupby = df_full.groupby(['bacno']).agg(\n",
    "    {'conam':['max','min','sum','count','mean','median']})\n",
    "\n",
    "df_full_groupby.columns = df_full_groupby.columns.droplevel()\n",
    "df_full_groupby.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_feature = df_full_groupby.columns.tolist()\n",
    "for i in range(len(col_feature[bycol_num:])):\n",
    "    col_feature[bycol_num+i] = col_feature[bycol_num+i] + '_bybacno'\n",
    "\n",
    "df_full_groupby.columns = col_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.merge(df_full_groupby, on = ['bacno'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Groupby bacno with other feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_FE = ['time_hour','cano','contp', 'etymd', 'mchno',\n",
    "          'acqic', 'mcc', 'stocn', 'scity', 'stscd', 'csmcu']\n",
    "bycol_num = 2\n",
    "\n",
    "for col in col_FE:\n",
    "    df_full_groupby = df_full.groupby(['bacno',col]).agg(\n",
    "        {'conam':['max','min','sum','count','mean','median']})\n",
    "    \n",
    "    df_full_groupby.columns = df_full_groupby.columns.droplevel()\n",
    "    df_full_groupby.reset_index(inplace = True)\n",
    "    col_feature = df_full_groupby.columns.tolist()\n",
    "    for i in range(len(col_feature[bycol_num:])):\n",
    "        col_feature[bycol_num+i] = col_feature[bycol_num+i] + '_bybacno_by' + col\n",
    "    \n",
    "    df_full_groupby.columns = col_feature\n",
    "    # Merge Data\n",
    "    df_full = df_full.merge(df_full_groupby, on = ['bacno',col], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_full_groupby = df_full.groupby(['bacno',col]).agg(\n",
    "    {'conam':['max','min','sum','count','mean','median']})\n",
    "\n",
    "df_full_groupby.columns = df_full_groupby.columns.droplevel()\n",
    "df_full_groupby.reset_index(inplace = True)\n",
    "col_feature = df_full_groupby.columns.tolist()\n",
    "for i in range(len(col_feature[bycol_num:])):\n",
    "    col_feature[bycol_num+i] = col_feature[bycol_num+i] + '_bybacno_by' + col\n",
    "\n",
    "df_full_groupby.columns = col_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_full = df_full.merge(df_full_groupby, on = ['bacno',col], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.4 Groupby bacno and stocn with other feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_FE = ['time_hour','cano','contp', 'etymd', 'mchno',\n",
    "          'acqic', 'mcc', 'scity', 'stscd', 'csmcu']\n",
    "bycol_num = 3\n",
    "\n",
    "\n",
    "for col in col_FE:\n",
    "    df_full_groupby = df_full.groupby(['bacno','stocn',col]).agg(\n",
    "        {'conam':['max','min','sum','count','mean','median']})\n",
    "    \n",
    "    df_full_groupby.columns = df_full_groupby.columns.droplevel()\n",
    "    df_full_groupby.reset_index(inplace = True)\n",
    "    col_feature = df_full_groupby.columns.tolist()\n",
    "    for i in range(len(col_feature[bycol_num:])):\n",
    "        col_feature[bycol_num+i] = col_feature[bycol_num+i] + '_bybacno_bystocn_by' + col\n",
    "    \n",
    "    df_full_groupby.columns = col_feature\n",
    "    \n",
    "    # Merge Data\n",
    "    df_full = df_full.merge(df_full_groupby, on = ['bacno','stocn',col], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 create new categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Frequency Encoding: \n",
    "Categorical Feature: ['contp', 'etymd', 'mchno', 'acqic', 'mcc', 'stocn', 'scity', 'stscd', 'csmcu'] 代表該值出現的頻率 by bacno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_encoding_features = ['time_hour','contp', 'etymd', 'mchno',\n",
    "                               'acqic', 'mcc', 'stocn', 'scity', 'stscd', 'csmcu']\n",
    "for col in frequency_encoding_features:\n",
    "    frequency = df_full.groupby(['bacno',col]).size() / df_full.groupby(['bacno']).size()\n",
    "    frequency = pd.DataFrame(frequency)\n",
    "    frequency.reset_index(inplace = True)\n",
    "    frequency.rename(columns = {0: 'fre_bacno_{}'.format(col)},inplace = True)\n",
    "    \n",
    "    df_full = df_full.merge(frequency, on = ['bacno',col], how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_encoding_features = ['time_hour','contp', 'etymd', 'mchno',\n",
    "                               'acqic', 'mcc', 'scity', 'stscd', 'csmcu']\n",
    "for col in frequency_encoding_features:\n",
    "    frequency = df_full.groupby(['stocn',col]).size() / df_full.groupby(['stocn']).size()\n",
    "    frequency = pd.DataFrame(frequency)\n",
    "    frequency.reset_index(inplace = True)\n",
    "    frequency.rename(columns = {0: 'fre_stocn_{}'.format(col)},inplace = True)\n",
    "    \n",
    "    df_full = df_full.merge(frequency, on = ['stocn',col], how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_encoding_features = ['time_hour','contp', 'etymd', 'mchno',\n",
    "                               'acqic', 'mcc', 'stocn' , 'stscd', 'csmcu']\n",
    "for col in frequency_encoding_features:\n",
    "    frequency = df_full.groupby(['scity',col]).size() / df_full.groupby(['scity']).size()\n",
    "    frequency = pd.DataFrame(frequency)\n",
    "    frequency.reset_index(inplace = True)\n",
    "    frequency.rename(columns = {0: 'fre_scity_{}'.format(col)},inplace = True)\n",
    "    \n",
    "    df_full = df_full.merge(frequency, on = ['scity',col], how ='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Mean encoding \n",
    "Categorical Feature: ['contp', 'etymd', 'mchno', 'acqic', 'mcc', 'stocn', 'scity', 'stscd', 'csmcu']   \n",
    "target:conam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean for all conam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_encoding_features = ['time_hour','contp', 'etymd', 'mchno',\n",
    "                          'acqic', 'mcc', 'stocn', 'scity', 'stscd', 'csmcu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mean_encoding_features:\n",
    "    # 舊版的FE是用df_train(from train data feature to test data)\n",
    "    mean_encoder = df_full.groupby([col])['conam'].mean()\n",
    "    df_full.loc[:, 'mean_encoding_{}'.format(col)] = df_full[col].map(mean_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean by stocn for all conam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_encoding_features = ['time_hour','contp', 'etymd', 'mchno',\n",
    "                          'acqic', 'mcc', 'scity', 'stscd', 'csmcu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mean_encoding_features:\n",
    "    # 舊版的FE是用df_train\n",
    "    mean_encoder = df_full.groupby(['stocn',col])['conam'].mean()\n",
    "    df_full.loc[:, 'mean_encoding_bystocn_{}'.format(col)] = df_full[col].map(mean_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean by scity for all conam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_encoding_features = ['time_hour','contp', 'etymd', 'mchno',\n",
    "                          'acqic', 'mcc','stocn', 'stscd', 'csmcu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mean_encoding_features:\n",
    "    # 舊版的FE是用df_train\n",
    "    mean_encoder = df_full.groupby(['scity',col])['conam'].mean()\n",
    "    df_full.loc[:, 'mean_encoding_byscity_{}'.format(col)] = df_full[col].map(mean_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.get_dummies(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN, 此处排除 fraud_ind\n",
    "for col in df_full.columns:\n",
    "    if col != 'fraud_ind':\n",
    "        df_full[col] = df_full[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['fraud_ind'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查有na的columns\n",
    "df_full.loc[:,df_full.columns[df_full.isnull().sum()>0]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.drop(['bacno','cano','locdt','loctm','time_sec','time_min','loctm_time',\n",
    "             ],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full.shape)\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_mart = df_full[: ntrain].copy()\n",
    "df_test_mart = df_full[ntrain:].copy()\n",
    "print(df_train_mart.shape,df_test_mart.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_mart.drop([col_target], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = df_train_mart.loc[:, df_train_mart.columns != col_target]\n",
    "df_train_y = df_train_mart.loc[:, df_train_mart.columns == col_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_X.shape)\n",
    "df_train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 xgbboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 0\n",
    "# NFOLDS = 10\n",
    "# #Kf = KFold(NFOLDS , random_state=SEED,shuffle=True)\n",
    "# Kf = StratifiedKFold(NFOLDS,random_state=SEED,shuffle=True)\n",
    "# x_train = df_train_X\n",
    "# y_train = df_train_y\n",
    "# x_test = df_test_mart\n",
    "# te_index ={}\n",
    "# y_te = {}\n",
    "# pred_pro = {}\n",
    "# meta_y = np.zeros((len(x_test),NFOLDS))\n",
    "# tStart = time.time()\n",
    "# i = 0\n",
    "# for train_index, test_index in Kf.split(x_train,y_train):\n",
    "#     te_index[i] = test_index\n",
    "#     x_tr = df_train_X.iloc[train_index]\n",
    "#     y_tr = df_train_y.iloc[train_index]\n",
    "#     x_te = df_train_X.iloc[test_index]\n",
    "#     y_te[i] = df_train_y.iloc[test_index]\n",
    "#     clf_cv = xgb.XGBClassifier(n_jobs=-1)\n",
    "#     clf_cv.fit(x_tr, y_tr)\n",
    "#     pred_pro[i] = clf_cv.predict_proba(x_te)[:,1]\n",
    "#     meta_y[:,i] = clf_cv.predict_proba(df_test_mart)[:,1]\n",
    "#     print('fold complete:',i+1)\n",
    "#     print('target ratio:',y_tr.mean())\n",
    "#     i = i+1\n",
    "\n",
    "# tEnd = time.time()\n",
    "# print('Time CV (s):',tEnd-tStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = np.arange(0.1,0.95,0.05)\n",
    "# f1_df_xgb = pd.DataFrame()\n",
    "# w = 0\n",
    "# for thres in threshold:\n",
    "#     for i in range(NFOLDS):\n",
    "#         f1_df_xgb.loc[w,'threshold'] = thres\n",
    "#         f1_df_xgb.loc[w,'f1_score'] = f1_score(y_te[i],np.where(pred_pro[i] > thres ,1,0))\n",
    "#         w = w + 1\n",
    "\n",
    "# f1_mean_xgb = f1_df_xgb.groupby('threshold').mean().reset_index().sort_values('f1_score',ascending=False).head(1)\n",
    "\n",
    "# f1_std_xgb = f1_df_xgb.groupby('threshold').std().reset_index().sort_values('f1_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('threshold:',f1_mean_xgb['threshold'].values)\n",
    "# print('mean:',f1_mean_xgb['f1_score'].values)\n",
    "# print('std:',f1_std_xgb.loc[f1_mean_xgb.index,'f1_score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = xgb.XGBClassifier(n_jobs=-1)\n",
    "# clf.fit(df_train_X, df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_pro4sub = clf.predict_proba(df_test_mart)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv catboost\n",
    "# 看似fit裡面只能餵df?\n",
    "\n",
    "SEED = 8787\n",
    "NFOLDS = 10\n",
    "cat_features = [0, 1]\n",
    "#Kf = KFold(NFOLDS , random_state=SEED,shuffle=True)\n",
    "Kf = StratifiedKFold(NFOLDS,random_state=SEED,shuffle=True)\n",
    "x_train = df_train_X\n",
    "y_train = df_train_y\n",
    "x_test = df_test_mart\n",
    "te_index ={}\n",
    "y_te = {}\n",
    "pred_pro = {}\n",
    "meta_y = np.zeros((len(x_test),NFOLDS))\n",
    "tStart = time.time()\n",
    "i = 0\n",
    "for train_index, test_index in Kf.split(x_train,y_train):\n",
    "    te_index[i] = test_index\n",
    "    x_tr = df_train_X.iloc[train_index]\n",
    "    y_tr = df_train_y.iloc[train_index]\n",
    "    x_te = df_train_X.iloc[test_index]\n",
    "    y_te[i] = df_train_y.iloc[test_index]\n",
    "    clf_cv = CatBoostClassifier(loss_function='Logloss',verbose=True)\n",
    "    clf_cv.fit(x_tr, y_tr)\n",
    "    pred_pro[i] = clf_cv.predict_proba(x_te)[:,1]\n",
    "    meta_y[:,i] = clf_cv.predict_proba(df_test_mart)[:,1]\n",
    "    print('fold complete:',i+1)\n",
    "    print('target ratio:',y_tr.mean())\n",
    "    i = i+1\n",
    "\n",
    "tEnd = time.time()\n",
    "print('Time CV (s):',tEnd-tStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.arange(0.1,0.95,0.05)\n",
    "f1_df_cat = pd.DataFrame()\n",
    "w = 0\n",
    "for thres in threshold:\n",
    "    for i in range(NFOLDS):\n",
    "        f1_df_cat.loc[w,'threshold'] = thres\n",
    "        f1_df_cat.loc[w,'f1_score'] = f1_score(y_te[i],np.where(pred_pro[i] > thres ,1,0))\n",
    "        w = w + 1\n",
    "\n",
    "f1_mean_cat = f1_df_cat.groupby('threshold').mean().reset_index().sort_values('f1_score',ascending=False).head(1)\n",
    "\n",
    "f1_std_cat = f1_df_cat.groupby('threshold').std().reset_index().sort_values('f1_score',ascending=False)\n",
    "# NFOLDS = 5\n",
    "# (loss_function='Logloss',verbose=True,scale_pos_weight=0.13)\n",
    "# threshold: [0.1]\n",
    "# mean: [0.71685114]\n",
    "# std: [0.0031393]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('threshold:',f1_mean_cat['threshold'].values)\n",
    "print('mean:',f1_mean_cat['f1_score'].values)\n",
    "print('std:',f1_std_cat.loc[f1_mean_cat.index,'f1_score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CatBoostClassifier(loss_function='Logloss',verbose=True)\n",
    "clf.fit(df_train_X, df_train_y)\n",
    "pred_pro4sub = clf.predict_proba(df_test_mart)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "features = df_train_X.columns\n",
    "\n",
    "plt.figure(figsize=(5,30)) # 調整長寬\n",
    "plt.title('Catboost Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), features[indices])\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import = pd.DataFrame({'colname':df_train_X.columns,'importances':clf.feature_importances_}\n",
    "            ).sort_values(by='importances',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = np.array([])\n",
    "idx = np.array([])\n",
    "for i in range(len(pred_pro)):\n",
    "    pro = np.append(pro, pred_pro[i].reshape(-1))\n",
    "    idx = np.append(idx, te_index[i])\n",
    "idx = idx.astype(int)\n",
    "oof = pd.DataFrame()\n",
    "oof['pro'] = pro\n",
    "oof.index = idx\n",
    "oof = oof.sort_index()\n",
    "oof['txkey'] = train['txkey'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof.to_csv('oof_date.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feature = pd.DataFrame()\n",
    "meta_feature['txkey'] = test['txkey']\n",
    "meta_feature['feature'] = np.mean(meta_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_feature.to_csv('meta_feature_date.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_y.fraud_ind.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "predictions =  np.where(pred_pro4sub > 0.35,1,0)\n",
    "pd.Series(predictions).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'txkey':df_test_id, 'fraud_ind': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_1111_v5_catbo.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
